{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import Libraries and dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n\ndf = pd.read_csv('/kaggle/input/nips-papers/papers.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:37:05.420189Z","iopub.execute_input":"2023-09-25T05:37:05.420611Z","iopub.status.idle":"2023-09-25T05:37:07.893635Z","shell.execute_reply.started":"2023-09-25T05:37:05.420575Z","shell.execute_reply":"2023-09-25T05:37:07.892269Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     id  year                                              title event_type  \\\n0     1  1987  Self-Organization of Associative Database and ...        NaN   \n1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n\n                                            pdf_name          abstract  \\\n0  1-self-organization-of-associative-database-an...  Abstract Missing   \n1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n\n                                          paper_text  \n0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n3  Bayesian Query Construction for Neural\\nNetwor...  \n4  Neural Network Ensembles, Cross\\nValidation, a...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>year</th>\n      <th>title</th>\n      <th>event_type</th>\n      <th>pdf_name</th>\n      <th>abstract</th>\n      <th>paper_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1987</td>\n      <td>Self-Organization of Associative Database and ...</td>\n      <td>NaN</td>\n      <td>1-self-organization-of-associative-database-an...</td>\n      <td>Abstract Missing</td>\n      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>1987</td>\n      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n      <td>NaN</td>\n      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n      <td>Abstract Missing</td>\n      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100</td>\n      <td>1988</td>\n      <td>Storing Covariance by the Associative Long-Ter...</td>\n      <td>NaN</td>\n      <td>100-storing-covariance-by-the-associative-long...</td>\n      <td>Abstract Missing</td>\n      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000</td>\n      <td>1994</td>\n      <td>Bayesian Query Construction for Neural Network...</td>\n      <td>NaN</td>\n      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n      <td>Abstract Missing</td>\n      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1001</td>\n      <td>1994</td>\n      <td>Neural Network Ensembles, Cross Validation, an...</td>\n      <td>NaN</td>\n      <td>1001-neural-network-ensembles-cross-validation...</td>\n      <td>Abstract Missing</td>\n      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"This dataset contains 7 columns: id, year, title, even_type, pdf_name, abstract and paper_text. We are mainly interested in the paper_text which includes both the title and the abstract.\n\nThe next step is to preprocess our textual data. For this task, I will use the NLTK library in Python:","metadata":{}},{"cell_type":"code","source":"# Download the WordNet corpus if not already downloaded\n# nltk.download('wordnet')\n# nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:40:31.198134Z","iopub.execute_input":"2023-09-25T05:40:31.198574Z","iopub.status.idle":"2023-09-25T05:40:31.207499Z","shell.execute_reply.started":"2023-09-25T05:40:31.198539Z","shell.execute_reply":"2023-09-25T05:40:31.206159Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"If you're running the notebook in kaggle use the below method to download wordnet","metadata":{}},{"cell_type":"code","source":"import nltk\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:45:08.400241Z","iopub.execute_input":"2023-09-25T05:45:08.400688Z","iopub.status.idle":"2023-09-25T05:45:09.030365Z","shell.execute_reply.started":"2023-09-25T05:45:08.400652Z","shell.execute_reply":"2023-09-25T05:45:09.029085Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /kaggle/working/...\nArchive:  /kaggle/working/corpora/wordnet.zip\n   creating: /kaggle/working/corpora/wordnet/\n  inflating: /kaggle/working/corpora/wordnet/lexnames  \n  inflating: /kaggle/working/corpora/wordnet/data.verb  \n  inflating: /kaggle/working/corpora/wordnet/index.adv  \n  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n  inflating: /kaggle/working/corpora/wordnet/index.verb  \n  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n  inflating: /kaggle/working/corpora/wordnet/data.adj  \n  inflating: /kaggle/working/corpora/wordnet/index.adj  \n  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n  inflating: /kaggle/working/corpora/wordnet/README  \n  inflating: /kaggle/working/corpora/wordnet/index.sense  \n  inflating: /kaggle/working/corpora/wordnet/data.noun  \n  inflating: /kaggle/working/corpora/wordnet/data.adv  \n  inflating: /kaggle/working/corpora/wordnet/index.noun  \n  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n# Creatin a list of custom stopwords\nnew_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\",\n            \"show\",\"result\",\"large\",\n            \"also\",\"one\",\"two\",\"three\",\n            \"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\"]\nstop_words = list(stop_words.union(new_words))\n\ndef pre_process(text):\n    \n    # lowercase\n    text = text.lower()\n    \n    # remove tags\n    text = re.sub(\"&lt:/?.*?&gt;\",\"&lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text = re.sub(\"(\\\\d|\\\\W)+\", \" \",text)\n    \n    # convert to list from string\n    text = text.split()\n    \n    # remove stopwords\n    text = [word for word in text if word not in stop_words]\n    \n    # remove words less than three letters\n    text = [word for word in text if len(word) >= 3]\n    \n    # lemmatize\n    lmtzr = WordNetLemmatizer()\n    text = [lmtzr.lemmatize(word) for word in text]\n    \n    return ' '.join(text)\ndocs = df['paper_text'].apply(lambda x:pre_process(x))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T05:45:19.670230Z","iopub.execute_input":"2023-09-25T05:45:19.670677Z","iopub.status.idle":"2023-09-25T05:49:41.772843Z","shell.execute_reply.started":"2023-09-25T05:45:19.670639Z","shell.execute_reply":"2023-09-25T05:49:41.771743Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Using TF-IDF\n\nTF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases in proportion to the number of times a word appears in the document (Text Frequency – TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency – IDF).\n\nUsing the tf-idf weighting scheme, the keywords are the words with the highest TF-IDF score. For this task, I’ll first use the CountVectorizer method in Scikit-learn to create a vocabulary and generate the word count:","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(max_df=0.95,          # ignore words that appear in 95% of documents\n                     max_features=10000,   # the size of the vocabulary\n                     ngram_range=(1,3)     # vocabulary contains single words, bigrams, trigrams\n                    )\nword_count_vector = cv.fit_transform(docs)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:02:15.618675Z","iopub.execute_input":"2023-09-25T06:02:15.619152Z","iopub.status.idle":"2023-09-25T06:05:31.704198Z","shell.execute_reply.started":"2023-09-25T06:02:15.619115Z","shell.execute_reply":"2023-09-25T06:05:31.702948Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Now I’m going to use the TfidfTransformer in Scikit-learn to calculate the reverse frequency of documents:","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:06:29.430431Z","iopub.execute_input":"2023-09-25T06:06:29.430830Z","iopub.status.idle":"2023-09-25T06:06:29.488849Z","shell.execute_reply.started":"2023-09-25T06:06:29.430795Z","shell.execute_reply":"2023-09-25T06:06:29.487447Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TfidfTransformer()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now, we are ready for the final step. In this step, I will create a function for the task of Keyword Extraction by using the Tf-IDF vectorization:","metadata":{}},{"cell_type":"code","source":"def sort_coo(coo_matric):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    # use only topn items from vector\n    sorted_items = sorted_items[:topn]\n    \n    score_vals = []\n    feature_vals = []\n    \n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n    \n    # create a tuples of feature,score\n    # results = zip(feature_vals, score_vals)\n    results = {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    return results\n\n# get feature names\nfeature_names=cv.get_feature_names()\n\ndef get_keywords(idx, docs):\n    \n    # generate tf-idf for given document\n    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n    \n    # sort the tf-idf vectors be descending order of scores\n    sorted_items = sort_coo(tf_idf_vector.tocoo())\n    \n    # extract only the top n; n here is 10\n    keywords=extract_topn_from_vector(feature_names, sorted_items,10)\n    \n    return keywords\n\ndef print_results(idx,keywords,df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    print(\"\\n=====Abstract=====\")\n    print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k,keywords[k])\nidx=10\nkeywords=get_keywords(idx, docs)\nprint_results(idx, keywords,df)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:23:18.440530Z","iopub.execute_input":"2023-09-25T06:23:18.441153Z","iopub.status.idle":"2023-09-25T06:23:18.505563Z","shell.execute_reply.started":"2023-09-25T06:23:18.441102Z","shell.execute_reply":"2023-09-25T06:23:18.504255Z"},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# get feature names\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m feature_names\u001b[38;5;241m=\u001b[39m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_keywords\u001b[39m(idx, docs):\n\u001b[1;32m     31\u001b[0m     \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# generate tf-idf for given document\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     tf_idf_vector\u001b[38;5;241m=\u001b[39mtfidf_transformer\u001b[38;5;241m.\u001b[39mtransform(cv\u001b[38;5;241m.\u001b[39mtransform([docs[idx]]))\n","\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"],"ename":"AttributeError","evalue":"'CountVectorizer' object has no attribute 'get_feature_names'","output_type":"error"}]},{"cell_type":"code","source":"def sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n\n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\n\n# get feature names\nfeature_names=cv.get_feature_names_out()\n\ndef get_keywords(idx, docs):\n\n    #generate tf-idf for the given document\n    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n\n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tf_idf_vector.tocoo())\n\n    #extract only the top n; n here is 10\n    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n    \n    return keywords\n\ndef print_results(idx,keywords, df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    print(\"\\n=====Abstract=====\")\n    print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k,keywords[k])\nidx=941\nkeywords=get_keywords(idx, docs)\nprint_results(idx,keywords, df)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T06:25:45.357844Z","iopub.execute_input":"2023-09-25T06:25:45.358379Z","iopub.status.idle":"2023-09-25T06:25:45.443586Z","shell.execute_reply.started":"2023-09-25T06:25:45.358332Z","shell.execute_reply":"2023-09-25T06:25:45.442363Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"\n=====Title=====\nAlgorithms for Non-negative Matrix Factorization\n\n=====Abstract=====\nNon-negative matrix factorization (NMF) has previously been shown to \nbe a useful decomposition for multivariate data. Two different multi- \nplicative algorithms for NMF are analyzed. They differ only slightly in \nthe multiplicative factor used in the update rules. One algorithm can be \nshown to minimize the conventional least squares error while the other \nminimizes the generalized Kullback-Leibler divergence. The monotonic \nconvergence of both algorithms can be proven using an auxiliary func- \ntion analogous to that used for proving convergence of the Expectation- \nMaximization algorithm. The algorithms can also be interpreted as diag- \nonally rescaled gradient descent, where the rescaling factor is optimally \nchosen to ensure convergence. \n\n===Keywords===\nupdate rule 0.344\nupdate 0.285\nauxiliary 0.212\nnon negative matrix 0.211\nnegative matrix 0.209\nrule 0.192\nnmf 0.183\nmultiplicative 0.176\nmatrix factorization 0.163\nmatrix 0.163\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Source:\n\nhttps://thecleverprogrammer.com/2020/12/01/keyword-extraction-with-python/","metadata":{}}]}