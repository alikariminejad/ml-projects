{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Dataset:\n\nhttps://www.kaggle.com/aashita/nyt-comments/download","metadata":{}},{"cell_type":"markdown","source":"Machine learning models for generating text can be used at the character, sentence, or even paragraph level. In this article, I’ll explain how to build a machine learning model to generate natural language text by implementing and training an advanced recurrent neural network using the Python programming language.","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries and Dataset","metadata":{}},{"cell_type":"code","source":"# from keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku\n\n# Set seeds for reproducibility\nfrom tensorflow.random import set_seed\nfrom numpy.random import seed\nset_seed(2)\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string, os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ncurr_dir = '/kaggle/input/nyt-comments/'  #dataset directory\nall_headlines = []\nfor filename in os.listdir(curr_dir):\n    if 'Articles' in filename:\n        article_df = pd.read_csv(curr_dir + filename)\n        all_headlines.extend(list(article_df.headline.values))\n        break\nall_headlines = [h for h in all_headlines if h != \"Unknown\"]\nlen(all_headlines)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:12:47.987966Z","iopub.execute_input":"2023-09-19T15:12:47.988371Z","iopub.status.idle":"2023-09-19T15:12:48.048411Z","shell.execute_reply.started":"2023-09-19T15:12:47.988339Z","shell.execute_reply":"2023-09-19T15:12:48.047452Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"829"},"metadata":{}}]},{"cell_type":"markdown","source":"### Data Preparation\n\nIn this step, I’ll first perform a data text cleanup that includes removing punctuation and lower case all words:","metadata":{}},{"cell_type":"code","source":"def clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt\n\ncorpus = [clean_text(x) for x in all_headlines]\ncorpus[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:12:48.050442Z","iopub.execute_input":"2023-09-19T15:12:48.050778Z","iopub.status.idle":"2023-09-19T15:12:48.068784Z","shell.execute_reply.started":"2023-09-19T15:12:48.050751Z","shell.execute_reply":"2023-09-19T15:12:48.067503Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['nfl vs politics has been battle all season long',\n 'voice vice veracity',\n 'a standups downward slide',\n 'new york today a groundhog has her day',\n 'a swimmers communion with the ocean',\n 'trail activity',\n 'super bowl',\n 'trumps mexican shakedown',\n 'pences presidential pet',\n 'fruit of a poison tree']"},"metadata":{}}]},{"cell_type":"markdown","source":"The next step is to generate sequences of N-gram tokens. The machine learning model of generating text requires a sequence of input data, because, given a sequence (of words/tokens), the goal is to predict the next word/token. For this task, we need to do some tokenization on the dataset.\n\nTokenization is a process of extracting tokens from a corpus. Python’s Keras library has a built-in tokenization model that can be used to get tokens and their index in the corpus. After this step, each text document in the dataset is converted into a sequence of tokens:","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    ## convert data to sequence of tokens\n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(corpus)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:12:48.070379Z","iopub.execute_input":"2023-09-19T15:12:48.071212Z","iopub.status.idle":"2023-09-19T15:12:48.126222Z","shell.execute_reply.started":"2023-09-19T15:12:48.071151Z","shell.execute_reply":"2023-09-19T15:12:48.124994Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Padding the Sequences\n\nNow that we have generated a dataset that contains the sequence of tokens, but be aware that different sequences can have different lengths. So, before we start training the text generation model, we need to fill in the sequences and make their lengths equal:","metadata":{}},{"cell_type":"code","source":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:12:48.128335Z","iopub.execute_input":"2023-09-19T15:12:48.128880Z","iopub.status.idle":"2023-09-19T15:12:48.175993Z","shell.execute_reply.started":"2023-09-19T15:12:48.128827Z","shell.execute_reply":"2023-09-19T15:12:48.174663Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Using LSTM for Text Generation\n\nUnlike other RNNs LSTMs have an additional state called “cell state” whereby the network makes adjustments in the flow of information. The advantage of this state is that the model can remember or forget the tilts more selectively. Now let’s train the LSTM model for the task of generating text with Python:","metadata":{}},{"cell_type":"code","source":"def create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    # Add Input Embedding Layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    # Add Hidden Layer 1 - LSTM Layer\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    \n    # Add Output Layer\n    model.add(Dense(total_words, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:12:48.179467Z","iopub.execute_input":"2023-09-19T15:12:48.179863Z","iopub.status.idle":"2023-09-19T15:12:48.583586Z","shell.execute_reply.started":"2023-09-19T15:12:48.179823Z","shell.execute_reply":"2023-09-19T15:12:48.582526Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_1 (Embedding)     (None, 16, 10)            22880     \n                                                                 \n lstm_1 (LSTM)               (None, 100)               44400     \n                                                                 \n dropout_1 (Dropout)         (None, 100)               0         \n                                                                 \n dense_1 (Dense)             (None, 2288)              231088    \n                                                                 \n=================================================================\nTotal params: 298,368\nTrainable params: 298,368\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's fit the model:","metadata":{}},{"cell_type":"code","source":"model.fit(predictors,label,epochs=100,verbose=5)","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:12:48.584794Z","iopub.execute_input":"2023-09-19T15:12:48.585127Z","iopub.status.idle":"2023-09-19T15:16:39.860196Z","shell.execute_reply.started":"2023-09-19T15:12:48.585098Z","shell.execute_reply":"2023-09-19T15:16:39.859013Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/100\nEpoch 2/100\nEpoch 3/100\nEpoch 4/100\nEpoch 5/100\nEpoch 6/100\nEpoch 7/100\nEpoch 8/100\nEpoch 9/100\nEpoch 10/100\nEpoch 11/100\nEpoch 12/100\nEpoch 13/100\nEpoch 14/100\nEpoch 15/100\nEpoch 16/100\nEpoch 17/100\nEpoch 18/100\nEpoch 19/100\nEpoch 20/100\nEpoch 21/100\nEpoch 22/100\nEpoch 23/100\nEpoch 24/100\nEpoch 25/100\nEpoch 26/100\nEpoch 27/100\nEpoch 28/100\nEpoch 29/100\nEpoch 30/100\nEpoch 31/100\nEpoch 32/100\nEpoch 33/100\nEpoch 34/100\nEpoch 35/100\nEpoch 36/100\nEpoch 37/100\nEpoch 38/100\nEpoch 39/100\nEpoch 40/100\nEpoch 41/100\nEpoch 42/100\nEpoch 43/100\nEpoch 44/100\nEpoch 45/100\nEpoch 46/100\nEpoch 47/100\nEpoch 48/100\nEpoch 49/100\nEpoch 50/100\nEpoch 51/100\nEpoch 52/100\nEpoch 53/100\nEpoch 54/100\nEpoch 55/100\nEpoch 56/100\nEpoch 57/100\nEpoch 58/100\nEpoch 59/100\nEpoch 60/100\nEpoch 61/100\nEpoch 62/100\nEpoch 63/100\nEpoch 64/100\nEpoch 65/100\nEpoch 66/100\nEpoch 67/100\nEpoch 68/100\nEpoch 69/100\nEpoch 70/100\nEpoch 71/100\nEpoch 72/100\nEpoch 73/100\nEpoch 74/100\nEpoch 75/100\nEpoch 76/100\nEpoch 77/100\nEpoch 78/100\nEpoch 79/100\nEpoch 80/100\nEpoch 81/100\nEpoch 82/100\nEpoch 83/100\nEpoch 84/100\nEpoch 85/100\nEpoch 86/100\nEpoch 87/100\nEpoch 88/100\nEpoch 89/100\nEpoch 90/100\nEpoch 91/100\nEpoch 92/100\nEpoch 93/100\nEpoch 94/100\nEpoch 95/100\nEpoch 96/100\nEpoch 97/100\nEpoch 98/100\nEpoch 99/100\nEpoch 100/100\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7e215eb244f0>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Testing the Text Generation Model\n\nOur machine learning model for the task of generating text is now ready. Next, let’s write the function to predict the next word based on the input words.","metadata":{}},{"cell_type":"markdown","source":"We will first tokenize the seed text, fill in the sequences, and move on to the trained model to get the predicted word. The multiple predicted words can be added together to obtain the predicted sequence:","metadata":{}},{"cell_type":"code","source":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predict_x=model.predict(token_list,verbose=0) \n        predicted=np.argmax(predict_x,axis=1)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()\n   \nprint (generate_text(\"united states\", 5, model, max_sequence_len))\nprint (generate_text(\"preident trump\", 4, model, max_sequence_len))\nprint (generate_text(\"donald trump\", 4, model, max_sequence_len))\nprint (generate_text(\"india and china\", 4, model, max_sequence_len))\nprint (generate_text(\"new york\", 4, model, max_sequence_len))\nprint (generate_text(\"science and technology\", 5, model, max_sequence_len))","metadata":{"execution":{"iopub.status.busy":"2023-09-19T15:30:57.202475Z","iopub.execute_input":"2023-09-19T15:30:57.203249Z","iopub.status.idle":"2023-09-19T15:30:58.977740Z","shell.execute_reply.started":"2023-09-19T15:30:57.203196Z","shell.execute_reply":"2023-09-19T15:30:58.976424Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"United States Race To Cement Their Priorities\nPreident Trump Is Perverse Fight In\nDonald Trump Is Numb It Survive\nIndia And China A Appetite Of Splitting\nNew York Today A Makeshift Law\nScience And Technology Interim Halt On Bathroom Use\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Source:\n\nhttps://thecleverprogrammer.com/2020/12/20/text-generation-with-python/#google_vignette","metadata":{}}]}